{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1744a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(path, messages=True):\n",
    "  import pandas as pd\n",
    "  df = pd.read_csv(path)\n",
    "  if messages: print(df.shape)\n",
    "  return df\n",
    "\n",
    "def bin_groups(df, features=[], cutoff=0.05, replace_with='Other', messages=True):\n",
    "  import pandas as pd\n",
    "  if len(features) == 0: features = df.columns\n",
    "  for feat in features:\n",
    "    if feat in df.columns:  # Make sure they don't accidentally enter a feature name that doesn't exist\n",
    "      if not pd.api.types.is_numeric_dtype(df[feat]):\n",
    "        other_list = df[feat].value_counts()[df[feat].value_counts() / df.shape[0] < cutoff].index\n",
    "        if len(other_list) > 0:\n",
    "            df.loc[df[feat].isin(other_list), feat] = replace_with\n",
    "            if messages and len(other_list) > 0: print(f'{feat} has been binned by setting {other_list.values} to {replace_with}')\n",
    "    else:\n",
    "      if messages: print(f'{feat} not found in the DataFrame provided. No binning performed')\n",
    "  return df\n",
    "\n",
    "def missing_drop(df, label, row_thresh=0.7, col_thresh=0.9, drop_all=False):\n",
    "  df.dropna(axis='rows', subset=[label], inplace=True)\n",
    "  df.dropna(axis='columns', thresh=1, inplace=True)\n",
    "  df.dropna(axis='rows', thresh=1, inplace=True)\n",
    "  df.dropna(axis='columns', thresh=round(df.shape[0] * row_thresh), inplace=True)\n",
    "  df.dropna(axis='rows', thresh=round(df.shape[1] * col_thresh), inplace=True)\n",
    "  if drop_all: df.dropna(axis='rows', inplace=True)\n",
    "  return df\n",
    "\n",
    "def Xandy(df, label):\n",
    "  import pandas as pd\n",
    "  y = df[label]\n",
    "  X = df.drop(columns=[label])\n",
    "  return X, y\n",
    "\n",
    "def dummy_code(X):\n",
    "  import pandas as pd\n",
    "  X = pd.get_dummies(X, drop_first=True)\n",
    "  return X\n",
    "\n",
    "def minmax(X):\n",
    "  import pandas as pd\n",
    "  from sklearn.preprocessing import MinMaxScaler\n",
    "  X = pd.DataFrame(MinMaxScaler().fit_transform(X.copy()), columns=X.columns, index=X.index)\n",
    "  return X\n",
    "\n",
    "def impute_KNN(df, label, neighbors=5):\n",
    "  from sklearn.impute import KNNImputer\n",
    "  import pandas as pd\n",
    "  X, y = Xandy(df, label)\n",
    "  X = dummy_code(X.copy())\n",
    "  X = minmax(X.copy())\n",
    "  imp = KNNImputer(n_neighbors=neighbors, weights=\"uniform\")\n",
    "  X = pd.DataFrame(imp.fit_transform(X), columns=X.columns, index=X.index)\n",
    "  return X.merge(y, left_index=True, right_index=True)\n",
    "\n",
    "def fit_cv_regression(df, k, label, repeat=True, algorithm='ensemble', random_state=1, messages=True):\n",
    "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "  import pandas as pd\n",
    "  from numpy import mean\n",
    "  X, y = Xandy(df, label)\n",
    "  X = dummy_code(X)\n",
    "  if repeat:  cv = RepeatedKFold(n_splits=k, n_repeats=5, random_state=random_state)\n",
    "  else:       cv = KFold(n_splits=k, random_state=random_state, shuffle=True)\n",
    "  if algorithm == 'linear':\n",
    "    from sklearn.linear_model import Ridge, LassoLars\n",
    "    model1 = Ridge(random_state=random_state)\n",
    "    model2 = LassoLars(random_state=random_state)\n",
    "    score1 = mean(cross_val_score(model1, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    score2 = mean(cross_val_score(model2, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  elif algorithm == 'ensemble':\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    model1 = RandomForestRegressor(random_state=random_state)\n",
    "    model2 = GradientBoostingRegressor(random_state=random_state)\n",
    "    score1 = mean(cross_val_score(model1, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    score2 = mean(cross_val_score(model2, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  else:\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    model1 = MLPRegressor(random_state=random_state, max_iter=10000)\n",
    "    model2 = KNeighborsRegressor()\n",
    "    score1 = mean(cross_val_score(model1, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    score2 = mean(cross_val_score(model2, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  if messages:\n",
    "    print('R2', '{: <25}'.format(type(model1).__name__), round(score1, 4))\n",
    "    print('R2', '{: <25}'.format(type(model2).__name__), round(score2, 4))\n",
    "  if score1 > score2: return model1.fit(X, y)\n",
    "  else:               return model2.fit(X, y)\n",
    "\n",
    "def select_features(df, label, model, max='auto'):\n",
    "  from sklearn.feature_selection import SelectFromModel\n",
    "  import pandas as pd\n",
    "  X, y = Xandy(df, label)\n",
    "  if max != 'auto':\n",
    "    sel = SelectFromModel(model, prefit=True, max_features=round(max*df.drop(columns=[label]).shape[1]))\n",
    "  else:\n",
    "    sel = SelectFromModel(model, prefit=True)\n",
    "  sel.transform(X)\n",
    "  columns = list(X.columns[sel.get_support()])\n",
    "  columns.append(label)\n",
    "  return df[columns]\n",
    "\n",
    "def dump_pickle(model, file_name):\n",
    "  import pickle\n",
    "  pickle.dump(model, open(file_name, \"wb\"))\n",
    "\n",
    "def load_pickle(file_name):\n",
    "  import pickle\n",
    "  model = pickle.load(open(file_name, \"rb\"))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e592a107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 RandomForestRegressor     0.962\n",
      "R2 GradientBoostingRegressor 0.9561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 RandomForestRegressor     0.9629\n",
      "R2 GradientBoostingRegressor 0.9584\n"
     ]
    }
   ],
   "source": [
    "# Import the data\n",
    "df = import_data('cbb.csv', messages=False)\n",
    "\n",
    "label = '3P_O'\n",
    "\n",
    "\n",
    "# Clean/prepare the data\n",
    "df = bin_groups(df, messages=False)\n",
    "df = missing_drop(df, label)\n",
    "df = impute_KNN(df, label)\n",
    "\n",
    "# Select features and store a trained model\n",
    "model = fit_cv_regression(df, k=10, label=label) # We have to begin with a trained model\n",
    "df_reduced = select_features(df.copy(), label, model) # Use that model to select features\n",
    "model = fit_cv_regression(df_reduced, k=10, label=label)  # Retrain the model with the smaller feature set\n",
    "\n",
    "# Deployment pipeline\n",
    "dump_pickle(model, 'saved_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "85f04114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         G        W    ADJOE    ADJDE  BARTHAG    EFG_O    EFG_D      TOR  \\\n",
      "0 1.000000 0.868421 0.889524 0.272500 0.968239 0.614679 0.414634 0.230263   \n",
      "1 1.000000 0.947368 1.000000 0.240000 0.991422 0.715596 0.395122 0.032895   \n",
      "2 1.000000 0.868421 0.720000 0.160000 0.952308 0.674312 0.395122 0.138158   \n",
      "3 0.942857 0.815789 0.735238 0.030000 0.985090 0.655963 0.165854 0.381579   \n",
      "4 0.971429 0.973684 0.784762 0.057500 0.988358 0.798165 0.073171 0.282895   \n",
      "\n",
      "      TORD      ORB      DRB      FTR     FTRD     2P_O     2P_D     3P_D  \\\n",
      "0 0.437158 0.900685 0.527273 0.325641 0.314480 0.615970 0.293617 0.594118   \n",
      "1 0.306011 0.606164 0.240909 0.425641 0.133484 0.650190 0.297872 0.670588   \n",
      "2 0.508197 0.380137 0.295455 0.284615 0.305430 0.646388 0.387234 0.417647   \n",
      "3 0.688525 0.445205 0.468182 0.341026 0.454751 0.574144 0.178723 0.211765   \n",
      "4 0.377049 0.534247 0.354545 0.497436 0.235294 0.707224 0.097872 0.170588   \n",
      "\n",
      "     ADJ_T      WAB     YEAR      3P_O  \n",
      "0 0.553435 0.882507 0.300000 32.700000  \n",
      "1 0.080153 0.953003 0.200000 36.500000  \n",
      "2 0.332061 0.838120 0.500000 35.200000  \n",
      "3 0.393130 0.840731 0.600000 36.500000  \n",
      "4 0.545802 0.859008 0.400000 38.200000  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a757259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.965442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>0.356597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>0.025266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          R-squared\n",
       "Bayesian   0.969708\n",
       "OLS        0.969708\n",
       "Ridge      0.965442\n",
       "LARS       0.494010\n",
       "Lasso      0.494010\n",
       "Poisson    0.356597\n",
       "Gamma      0.025266\n",
       "Inverse    0.000041"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppress scientific notation in pandas\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "\n",
    "fit = {}         # Use this to store each of the fit metrics\n",
    "models = {}      # Use this to store each of the models\n",
    "random_state = 1 # Updates all models and cross-validators at once\n",
    "\n",
    "# 1. LINEAR MODELS: assumes normal distribution, homoscedasticity, no multi-collinearity, independence, and no auto-correlation (some exceptions apply; some of these algorithms are better at handling violations of these assumptions)\n",
    "import sklearn.linear_model as lm, pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from numpy import mean\n",
    "\n",
    "# Let's use the reduced feature set we established in our pipeline\n",
    "X, y = Xandy(df_reduced, label)\n",
    "\n",
    "# Set up a standard cross_validation object to use for each algorithm\n",
    "cv = KFold(n_splits=5, random_state=random_state, shuffle=True)\n",
    "\n",
    "# 1.1. Ordinary Least Squares Multiple Linear Regression\n",
    "model_ols = lm.LinearRegression()\n",
    "fit['OLS'] = mean(cross_val_score(model_ols, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['OLS'] = model_ols\n",
    "\n",
    "# 1.2. Ridge Regression: more robust to multi-collinearity\n",
    "model_rr = lm.Ridge(alpha=0.5, random_state=random_state) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "fit['Ridge'] = mean(cross_val_score(model_rr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Ridge'] = model_rr\n",
    "\n",
    "# 1.3. Lasso Regression: better for sparse values like RetweetCount where most are zeros but a few have many retweets.\n",
    "model_lr = lm.Lasso(alpha=0.1, random_state=random_state) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "fit['Lasso'] = mean(cross_val_score(model_lr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Lasso'] = model_lr\n",
    "\n",
    "# 1.4. Least Angle Regression: good when the number of features is greater than the number of samples\n",
    "model_llr = lm.LassoLars(alpha=0.1, random_state=random_state) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "fit['LARS'] = mean(cross_val_score(model_llr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['LARS'] = model_llr\n",
    "\n",
    "# 1.5. Bayesian Regression: probability based; allows regularization parameters, automatically tuned to data\n",
    "model_br = lm.BayesianRidge()\n",
    "fit['Bayesian'] = mean(cross_val_score(model_br, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Bayesian'] = model_br\n",
    "\n",
    "# 1.6. Generalized Linear Regression (Poisson): Good for non-normal distribution, count-based data, and a Poisson distribution\n",
    "model_pr = lm.TweedieRegressor(power=1, link=\"log\") # Power=1 means this is a Poisson\n",
    "fit['Poisson'] = mean(cross_val_score(model_pr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Poisson'] = model_pr\n",
    "\n",
    "# 1.7. Generalized Linear Regression (Gamma): Good for non-normal distribution, continuous data, and a Gamma distribution\n",
    "model_gr = lm.TweedieRegressor(power=2, link=\"log\") # Power=2 means this is a Gamma\n",
    "fit['Gamma'] = mean(cross_val_score(model_gr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Gamma'] = model_gr\n",
    "\n",
    "# 1.8. Generalized Linear Regression (Inverse Gamma): Good non-normal distribution, continuous data, and an inverse Gamma distribution\n",
    "model_igr = lm.TweedieRegressor(power=3) # Power=3 means this is an inverse Gamma\n",
    "fit['Inverse'] = mean(cross_val_score(model_igr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Inverse'] = model_igr\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a51a1044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NuSupportVM</th>\n",
       "      <td>0.970433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupportVM</th>\n",
       "      <td>0.970293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.965442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM</th>\n",
       "      <td>0.962009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>0.356597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>0.025266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             R-squared\n",
       "NuSupportVM   0.970433\n",
       "SupportVM     0.970293\n",
       "Bayesian      0.969708\n",
       "OLS           0.969708\n",
       "Ridge         0.965442\n",
       "Linear SVM    0.962009\n",
       "LARS          0.494010\n",
       "Lasso         0.494010\n",
       "Poisson       0.356597\n",
       "Gamma         0.025266\n",
       "Inverse       0.000041"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SUPPORT VECTOR MACHINES: Ideal for noisy data with large gaps among values\n",
    "from sklearn import svm\n",
    "\n",
    "# 1.9. SVM: this is the default SVM, parameters can be modified to make this more accurate\n",
    "model_svm = svm.SVR()\n",
    "fit['SupportVM'] = mean(cross_val_score(model_svm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['SupportVM'] = model_svm\n",
    "\n",
    "# 1.10. Linear SVM: Faster than SVM but only considers a linear model\n",
    "model_lsvm = svm.LinearSVR(random_state=random_state)\n",
    "fit['Linear SVM'] = mean(cross_val_score(model_lsvm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Linear SVM'] = model_lsvm\n",
    "\n",
    "# 1.11. NuSVM:\n",
    "model_nusvm = svm.NuSVR()\n",
    "fit['NuSupportVM'] = mean(cross_val_score(model_nusvm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['NuSupportVM'] = model_nusvm\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5ac531b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NuSupportVM</th>\n",
       "      <td>0.970433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupportVM</th>\n",
       "      <td>0.970293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.965442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighbors</th>\n",
       "      <td>0.963626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM</th>\n",
       "      <td>0.962009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighborsD</th>\n",
       "      <td>0.960788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>0.356597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>0.025266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              R-squared\n",
       "NuSupportVM    0.970433\n",
       "SupportVM      0.970293\n",
       "Bayesian       0.969708\n",
       "OLS            0.969708\n",
       "Ridge          0.965442\n",
       "KNNeighbors    0.963626\n",
       "Linear SVM     0.962009\n",
       "KNNeighborsD   0.960788\n",
       "LARS           0.494010\n",
       "Lasso          0.494010\n",
       "Poisson        0.356597\n",
       "Gamma          0.025266\n",
       "Inverse        0.000041"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN: NEAREST NEIGHBORS REGRESSION\n",
    "from sklearn import neighbors\n",
    "\n",
    "# 1.12. KNeighborsRegressor:\n",
    "model_knnr = neighbors.KNeighborsRegressor(n_neighbors=10, weights='uniform')\n",
    "fit['KNNeighbors'] = mean(cross_val_score(model_knnr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['KNNeighbors'] = model_knnr\n",
    "\n",
    "# 1.13. KNeighborsRegressor:\n",
    "model_knnrd = neighbors.KNeighborsRegressor(n_neighbors=10, weights='distance')\n",
    "fit['KNNeighborsD'] = mean(cross_val_score(model_knnrd, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['KNNeighborsD'] = model_knnrd\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0808605a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NuSupportVM</th>\n",
       "      <td>0.970433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupportVM</th>\n",
       "      <td>0.970293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.965442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighbors</th>\n",
       "      <td>0.963626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM</th>\n",
       "      <td>0.962009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighborsD</th>\n",
       "      <td>0.960788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianP</th>\n",
       "      <td>0.955236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>0.356597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>0.025266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              R-squared\n",
       "NuSupportVM    0.970433\n",
       "SupportVM      0.970293\n",
       "Bayesian       0.969708\n",
       "OLS            0.969708\n",
       "Ridge          0.965442\n",
       "KNNeighbors    0.963626\n",
       "Linear SVM     0.962009\n",
       "KNNeighborsD   0.960788\n",
       "GaussianP      0.955236\n",
       "LARS           0.494010\n",
       "Lasso          0.494010\n",
       "Poisson        0.356597\n",
       "Gamma          0.025266\n",
       "Inverse        0.000041"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GAUSSIAN PROCESS REGRESSION\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "# 1.14. GaussianProcessRegressor:\n",
    "model_gpr = gaussian_process.GaussianProcessRegressor(DotProduct() + WhiteKernel(), random_state=random_state)\n",
    "fit['GaussianP'] = mean(cross_val_score(model_gpr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['GaussianP'] = model_gpr\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fddf8413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NuSupportVM</th>\n",
       "      <td>0.970433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupportVM</th>\n",
       "      <td>0.970293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stacking</th>\n",
       "      <td>0.966016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.965442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG Boost</th>\n",
       "      <td>0.963908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighbors</th>\n",
       "      <td>0.963626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dec Forest</th>\n",
       "      <td>0.962021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM</th>\n",
       "      <td>0.962009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighborsD</th>\n",
       "      <td>0.960788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>0.959407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voting</th>\n",
       "      <td>0.958700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grad. Boost</th>\n",
       "      <td>0.957759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianP</th>\n",
       "      <td>0.955236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dec Tree</th>\n",
       "      <td>0.942077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost DT</th>\n",
       "      <td>0.875146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>0.356597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>0.025266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              R-squared\n",
       "NuSupportVM    0.970433\n",
       "SupportVM      0.970293\n",
       "Bayesian       0.969708\n",
       "OLS            0.969708\n",
       "Stacking       0.966016\n",
       "Ridge          0.965442\n",
       "HG Boost       0.963908\n",
       "KNNeighbors    0.963626\n",
       "Dec Forest     0.962021\n",
       "Linear SVM     0.962009\n",
       "KNNeighborsD   0.960788\n",
       "Extra Trees    0.959407\n",
       "Voting         0.958700\n",
       "Grad. Boost    0.957759\n",
       "GaussianP      0.955236\n",
       "Dec Tree       0.942077\n",
       "AdaBoost DT    0.875146\n",
       "LARS           0.494010\n",
       "Lasso          0.494010\n",
       "Poisson        0.356597\n",
       "Gamma          0.025266\n",
       "Inverse        0.000041"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DECISION TREE MODELS: no assumptions about the data\n",
    "import sklearn.tree as tree\n",
    "import sklearn.ensemble as se\n",
    "\n",
    "# 1.15. Decision Tree Regression\n",
    "model_dt = tree.DecisionTreeRegressor(random_state=random_state)\n",
    "fit['Dec Tree'] = mean(cross_val_score(model_dt, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Dec Tree'] = model_dt\n",
    "\n",
    "\n",
    "# DECISION TREE-BASED ENSEMBLE MODELS: great for minimizing overfitting, these are based on averaging many unique sub-samples and combining algorithms\n",
    "# 1.16. Decision Forrest\n",
    "model_df = se.RandomForestRegressor(random_state=random_state)\n",
    "fit['Dec Forest'] = mean(cross_val_score(model_df, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Dec Forest'] = model_df\n",
    "\n",
    "# 1.17. ExtraTreesRegressor\n",
    "model_etr = se.ExtraTreesRegressor(random_state=random_state)\n",
    "fit['Extra Trees'] = mean(cross_val_score(model_etr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Extra Trees'] = model_etr\n",
    "\n",
    "# 1.18. AdaBoostRegressor\n",
    "model_abr = se.AdaBoostRegressor(n_estimators=100, random_state=random_state)\n",
    "fit['AdaBoost DT'] = mean(cross_val_score(model_abr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['AdaBoost DT'] = model_abr\n",
    "\n",
    "# 1.19. GradientBoostingRegressor\n",
    "model_gbr = se.GradientBoostingRegressor(random_state=random_state)\n",
    "fit['Grad. Boost'] = mean(cross_val_score(model_gbr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Grad. Boost'] = model_gbr\n",
    "\n",
    "# 1.20. HistGradientBoostingRegressor\n",
    "model_hgbr = se.HistGradientBoostingRegressor(random_state=random_state)\n",
    "fit['HG Boost'] = mean(cross_val_score(model_hgbr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['HG Boost'] = model_hgbr\n",
    "\n",
    "# 1.21. VotingRegressor: will combine other algorithms into an average; kind of cool\n",
    "model_vr = se.VotingRegressor(estimators=[('DT', model_dt), ('DF', model_df), ('ETR', model_etr), ('ABR', model_abr), ('GBR', model_gbr)])\n",
    "fit['Voting'] = mean(cross_val_score(model_vr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Voting'] = model_vr\n",
    "\n",
    "# 1.22. StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "estimators = [('ridge', RidgeCV()), ('lasso', LassoCV(random_state=42)), ('svr', svm.SVR(C=1, gamma=1e-6))]\n",
    "model_sr = se.StackingRegressor(estimators=estimators, final_estimator=se.GradientBoostingRegressor(random_state=random_state))\n",
    "fit['Stacking'] = mean(cross_val_score(model_sr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Stacking'] = model_sr\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b534944f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NuSupportVM</th>\n",
       "      <td>0.970433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupportVM</th>\n",
       "      <td>0.970293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stacking</th>\n",
       "      <td>0.966016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.965442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG Boost</th>\n",
       "      <td>0.963908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighbors</th>\n",
       "      <td>0.963626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dec Forest</th>\n",
       "      <td>0.962021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM</th>\n",
       "      <td>0.962009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighborsD</th>\n",
       "      <td>0.960788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>0.959407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voting</th>\n",
       "      <td>0.958700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grad. Boost</th>\n",
       "      <td>0.957759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianP</th>\n",
       "      <td>0.955236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dec Tree</th>\n",
       "      <td>0.942077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.916475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost DT</th>\n",
       "      <td>0.875146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>0.356597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>0.025266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              R-squared\n",
       "NuSupportVM    0.970433\n",
       "SupportVM      0.970293\n",
       "Bayesian       0.969708\n",
       "OLS            0.969708\n",
       "Stacking       0.966016\n",
       "Ridge          0.965442\n",
       "HG Boost       0.963908\n",
       "KNNeighbors    0.963626\n",
       "Dec Forest     0.962021\n",
       "Linear SVM     0.962009\n",
       "KNNeighborsD   0.960788\n",
       "Extra Trees    0.959407\n",
       "Voting         0.958700\n",
       "Grad. Boost    0.957759\n",
       "GaussianP      0.955236\n",
       "Dec Tree       0.942077\n",
       "XGBoost        0.916475\n",
       "AdaBoost DT    0.875146\n",
       "LARS           0.494010\n",
       "Lasso          0.494010\n",
       "Poisson        0.356597\n",
       "Gamma          0.025266\n",
       "Inverse        0.000041"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 1.23. XGBRegressor\n",
    "model_xgb = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8, random_state=random_state)\n",
    "fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['XGBoost'] = model_xgb\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d98bb7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NeuralNet</th>\n",
       "      <td>0.972002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSupportVM</th>\n",
       "      <td>0.970433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupportVM</th>\n",
       "      <td>0.970293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.969708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stacking</th>\n",
       "      <td>0.966016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.965442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG Boost</th>\n",
       "      <td>0.963908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighbors</th>\n",
       "      <td>0.963626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dec Forest</th>\n",
       "      <td>0.962021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM</th>\n",
       "      <td>0.962009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighborsD</th>\n",
       "      <td>0.960788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>0.959407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voting</th>\n",
       "      <td>0.958700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grad. Boost</th>\n",
       "      <td>0.957759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianP</th>\n",
       "      <td>0.955236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dec Tree</th>\n",
       "      <td>0.942077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.916475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost DT</th>\n",
       "      <td>0.875146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.494010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>0.356597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>0.025266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              R-squared\n",
       "NeuralNet      0.972002\n",
       "NuSupportVM    0.970433\n",
       "SupportVM      0.970293\n",
       "Bayesian       0.969708\n",
       "OLS            0.969708\n",
       "Stacking       0.966016\n",
       "Ridge          0.965442\n",
       "HG Boost       0.963908\n",
       "KNNeighbors    0.963626\n",
       "Dec Forest     0.962021\n",
       "Linear SVM     0.962009\n",
       "KNNeighborsD   0.960788\n",
       "Extra Trees    0.959407\n",
       "Voting         0.958700\n",
       "Grad. Boost    0.957759\n",
       "GaussianP      0.955236\n",
       "Dec Tree       0.942077\n",
       "XGBoost        0.916475\n",
       "AdaBoost DT    0.875146\n",
       "LARS           0.494010\n",
       "Lasso          0.494010\n",
       "Poisson        0.356597\n",
       "Gamma          0.025266\n",
       "Inverse        0.000041"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEURAL-NETWORK MODELS: Based on deep learning methods\n",
    "import sklearn.neural_network as nn\n",
    "\n",
    "# 1.23. MLPRegressor\n",
    "model_nn = nn.MLPRegressor(max_iter=1000, random_state=random_state) # Turn max_iter way up or down to get a more accurate result\n",
    "fit['NeuralNet'] = mean(cross_val_score(model_nn, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['NeuralNet'] = model_nn\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "82da2436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cv_regression_expanded(df, label, k=10, r=5, repeat=True, random_state=1):\n",
    "  import sklearn.linear_model as lm, pandas as pd, sklearn.ensemble as se\n",
    "  import sklearn.neural_network as nn\n",
    "  import sklearn.neighbors as neighbors\n",
    "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "  from numpy import mean, std\n",
    "  from sklearn import svm\n",
    "  from sklearn import gaussian_process\n",
    "  from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "  from xgboost import XGBRegressor\n",
    "\n",
    "  X, y = Xandy(df, label)\n",
    "\n",
    "  if repeat:\n",
    "    cv = RepeatedKFold(n_splits=k, n_repeats=r, random_state=random_state)\n",
    "  else:\n",
    "    cv = KFold(n_splits=k, random_state=random_state, shuffle=True)\n",
    "\n",
    "  fit = {}    # Use this to store each of the fit metrics\n",
    "  models = {} # Use this to store each of the models\n",
    "\n",
    "  # Create the model objects\n",
    "  model_ols = lm.LinearRegression()\n",
    "  model_rr = lm.Ridge(alpha=0.5, random_state=random_state) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "  model_lr = lm.Lasso(alpha=0.1, random_state=random_state) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "  model_llr = lm.LassoLars(alpha=0.1, random_state=random_state) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "  model_br = lm.BayesianRidge()\n",
    "  model_pr = lm.TweedieRegressor(power=1, link=\"log\") # Power=1 means this is a Poisson\n",
    "  model_gr = lm.TweedieRegressor(power=2, link=\"log\") # Power=2 means this is a Gamma\n",
    "  model_igr = lm.TweedieRegressor(power=3) # Power=3 means this is an inverse Gamma\n",
    "  model_svm = svm.SVR()\n",
    "  model_lsvm = svm.LinearSVR(random_state=random_state)\n",
    "  model_nusvm = svm.NuSVR()\n",
    "  model_knnr = neighbors.KNeighborsRegressor(n_neighbors=10, weights='uniform')\n",
    "  model_knnrd = neighbors.KNeighborsRegressor(n_neighbors=10, weights='distance')\n",
    "  model_gpr = gaussian_process.GaussianProcessRegressor(DotProduct() + WhiteKernel(), random_state=random_state)\n",
    "  model_df = se.RandomForestRegressor(random_state=random_state)\n",
    "  model_etr = se.ExtraTreesRegressor(random_state=random_state)\n",
    "  model_abr = se.AdaBoostRegressor(n_estimators=100, random_state=random_state)\n",
    "  model_gbr = se.GradientBoostingRegressor(random_state=random_state)\n",
    "  model_hgbr = se.HistGradientBoostingRegressor(random_state=random_state)\n",
    "  model_vr = se.VotingRegressor(estimators=[('DF', model_df), ('ETR', model_etr), ('ABR', model_abr), ('GBR', model_gbr)])\n",
    "  estimators = [('ridge', lm.RidgeCV()), ('lasso', lm.LassoCV(random_state=42)), ('svr', svm.SVR(C=1, gamma=1e-6))]\n",
    "  model_sr = se.StackingRegressor(estimators=estimators, final_estimator=se.GradientBoostingRegressor(random_state=random_state))\n",
    "  model_xgb = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8, random_state=random_state)\n",
    "  model_nn = nn.MLPRegressor(max_iter=1000, random_state=random_state)\n",
    "\n",
    "  # Fit a crss-validated R squared score and add it to the dict\n",
    "  fit['OLS'] = mean(cross_val_score(model_ols, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Ridge'] = mean(cross_val_score(model_rr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Lasso'] = mean(cross_val_score(model_lr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['LARS'] = mean(cross_val_score(model_llr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Bayesian'] = mean(cross_val_score(model_br, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Poisson'] = mean(cross_val_score(model_pr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Gamma'] = mean(cross_val_score(model_gr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Inverse'] = mean(cross_val_score(model_igr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['SupportVM'] = mean(cross_val_score(model_svm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Linear SVM'] = mean(cross_val_score(model_lsvm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['NuSupportVM'] = mean(cross_val_score(model_nusvm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['KNNeighbors'] = mean(cross_val_score(model_knnr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['KNNeighborsD'] = mean(cross_val_score(model_knnrd, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['GaussianP'] = mean(cross_val_score(model_gpr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Dec Forest'] = mean(cross_val_score(model_df, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Extra Trees'] = mean(cross_val_score(model_etr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['AdaBoost DT'] = mean(cross_val_score(model_abr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Grad. Boost'] = mean(cross_val_score(model_gbr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['HG Boost'] = mean(cross_val_score(model_hgbr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Voting'] = mean(cross_val_score(model_vr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['Stacking'] = mean(cross_val_score(model_sr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  fit['NeuralNet'] = mean(cross_val_score(model_nn, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "\n",
    "  # Add the model to another dict; make sure the keys have the same names as the list above\n",
    "  models['OLS'] = model_ols\n",
    "  models['Ridge'] = model_rr\n",
    "  models['Lasso'] = model_lr\n",
    "  models['LARS'] = model_llr\n",
    "  models['Bayesian'] = model_br\n",
    "  models['Poisson'] = model_pr\n",
    "  models['Gamma'] = model_gr\n",
    "  models['Inverse'] = model_igr\n",
    "  models['SupportVM'] = model_svm\n",
    "  models['Linear SVM'] = model_lsvm\n",
    "  models['NuSupportVM'] = model_nusvm\n",
    "  models['KNNeighbors'] = model_knnr\n",
    "  models['KNNeighborsD'] = model_knnrd\n",
    "  models['GaussianP'] = model_gpr\n",
    "  models['Dec Forest'] = model_df\n",
    "  models['Extra Trees'] = model_etr\n",
    "  models['AdaBoost DT'] = model_abr\n",
    "  models['Grad. Boost'] = model_gbr\n",
    "  models['HG Boost'] = model_hgbr\n",
    "  models['Voting'] = model_vr\n",
    "  models['Stacking'] = model_sr\n",
    "  models['XGBoost'] = model_xgb\n",
    "  models['NeuralNet'] = model_nn\n",
    "\n",
    "  # Add the fit dictionary to a new DataFrame, sort, extract the top row, use it to retrieve the model object from the models dictionary\n",
    "  df_fit = pd.DataFrame({'R-squared':fit})\n",
    "  df_fit.sort_values(by=['R-squared'], ascending=False, inplace=True)\n",
    "  best_model = df_fit.index[0]\n",
    "  print(df_fit)\n",
    "\n",
    "  return models[best_model].fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aedc486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              R-squared\n",
      "NeuralNet      0.971509\n",
      "NuSupportVM    0.970849\n",
      "SupportVM      0.970823\n",
      "Bayesian       0.969551\n",
      "OLS            0.969551\n",
      "Ridge          0.966125\n",
      "Stacking       0.965381\n",
      "HG Boost       0.964697\n",
      "KNNeighbors    0.964410\n",
      "Linear SVM     0.963451\n",
      "Dec Forest     0.962878\n",
      "KNNeighborsD   0.960862\n",
      "Extra Trees    0.959069\n",
      "Grad. Boost    0.958397\n",
      "GaussianP      0.957720\n",
      "Voting         0.957536\n",
      "XGBoost        0.925877\n",
      "AdaBoost DT    0.872176\n",
      "LARS           0.493122\n",
      "Lasso          0.493122\n",
      "Poisson        0.355925\n",
      "Gamma          0.023972\n",
      "Inverse       -0.001352\n"
     ]
    }
   ],
   "source": [
    "# Don't forget to mount Google Drive if you need it:\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Setting the label here since it is used in multiple function calls\n",
    "label = '3P_O'\n",
    "\n",
    "# Import the data\n",
    "df = import_data('cbb.csv', messages=False)\n",
    "\n",
    "# Clean/prepare the data\n",
    "df = bin_groups(df, messages=False)\n",
    "df = missing_drop(df, label)\n",
    "df = impute_KNN(df, label)\n",
    "\n",
    "# Select features and store a trained model\n",
    "model = fit_cv_regression(df, 10, label, messages=False) # We have to begin with a trained model\n",
    "df_reduced = select_features(df.copy(), label, model) # Use that model to select features\n",
    "model = fit_cv_regression_expanded(df_reduced, label, k=10, r=5)  # Retrain the model with the smaller feature set\n",
    "\n",
    "# Deployment pipeline\n",
    "dump_pickle(model, 'saved_model_1.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "228b1a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Actual 3P_O  Predicted 3P_O  Difference\n",
      "3518    39.700000       39.351392    0.348608\n",
      "3519    36.400000       35.983525    0.416475\n",
      "3520    33.400000       33.583427   -0.183427\n",
      "3521    35.600000       35.361983    0.238017\n",
      "3522    35.700000       36.064415   -0.364415\n"
     ]
    }
   ],
   "source": [
    "# Later when a page loads that needs the predicted value(s):\n",
    "import pandas as pd\n",
    "model = load_pickle('saved_model_1.sav')\n",
    "df_predictions = pd.DataFrame({'Actual 3P_O':df_reduced[\"3P_O\"],\n",
    "                               'Predicted 3P_O':model.predict(df_reduced.drop(columns=['3P_O']))})\n",
    "df_predictions['Difference'] = df_predictions['Actual 3P_O'] - df_predictions['Predicted 3P_O']\n",
    "print(df_predictions.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a9ef342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cv_classification_expanded(df, label, k=10, r=5, repeat=True, random_state=1):\n",
    "  import sklearn.linear_model as lm, pandas as pd, sklearn.ensemble as se, numpy as np\n",
    "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "  from numpy import mean, std\n",
    "  from sklearn import svm\n",
    "  from sklearn import gaussian_process\n",
    "  from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "  from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "  from sklearn import svm\n",
    "  from sklearn.naive_bayes import CategoricalNB\n",
    "  from xgboost import XGBClassifier\n",
    "  from sklearn import preprocessing\n",
    "  from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "  X, y = Xandy(df, label)\n",
    "\n",
    "  if repeat:\n",
    "    cv = RepeatedKFold(n_splits=k, n_repeats=r, random_state=random_state)\n",
    "  else:\n",
    "    cv = KFold(n_splits=k, random_state=random_state, shuffle=True)\n",
    "\n",
    "  fit = {}    # Use this to store each of the fit metrics\n",
    "  models = {} # Use this to store each of the models\n",
    "\n",
    "  # Create the model objects\n",
    "  model_log = lm.LogisticRegression(max_iter=100)\n",
    "  model_logcv = lm.RidgeClassifier()\n",
    "  model_sgd = lm.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "  model_pa = lm.PassiveAggressiveClassifier(max_iter=1000, random_state=random_state, tol=1e-3)\n",
    "  model_per = lm.Perceptron(fit_intercept=False, max_iter=10, tol=None, shuffle=False)\n",
    "  model_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "  model_svm = svm.SVC(decision_function_shape='ovo') # Remove the parameter for two-class model\n",
    "  model_nb = CategoricalNB()\n",
    "  model_bag = se.BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "  model_ada = se.AdaBoostClassifier(n_estimators=100, random_state=random_state)\n",
    "  model_ext = se.ExtraTreesClassifier(n_estimators=100, random_state=random_state)\n",
    "  model_rf = se.RandomForestClassifier(n_estimators=10)\n",
    "  model_hgb = se.HistGradientBoostingClassifier(max_iter=100)\n",
    "  model_vot = se.VotingClassifier(estimators=[('lr', model_log), ('rf', model_ext), ('gnb', model_hgb)], voting='hard')\n",
    "  model_gb = se.GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "  estimators = [('ridge', lm.RidgeCV()), ('lasso', lm.LassoCV(random_state=random_state)), ('knr', KNeighborsRegressor(n_neighbors=20, metric='euclidean'))]\n",
    "  final_estimator = se.GradientBoostingRegressor(n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1, random_state=random_state)\n",
    "  model_st = se.StackingRegressor(estimators=estimators, final_estimator=final_estimator)\n",
    "  model_nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=random_state)\n",
    "  model_xgb = XGBClassifier()\n",
    "\n",
    "  # Fit a crss-validated R squared score and add it to the dict\n",
    "  fit['Logistic'] = mean(cross_val_score(model_log, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Ridge'] = mean(cross_val_score(model_logcv, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['SGD'] = mean(cross_val_score(model_sgd, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['PassiveAggressive'] = mean(cross_val_score(model_pa, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Perceptron'] = mean(cross_val_score(model_per, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['KNN'] = mean(cross_val_score(model_knn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['SVM'] = mean(cross_val_score(model_svm, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['NaiveBayes'] = mean(cross_val_score(model_nb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Bagging'] = mean(cross_val_score(model_bag, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['AdaBoost'] = mean(cross_val_score(model_ada, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['ExtraTrees'] = mean(cross_val_score(model_ext, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['RandomForest'] = mean(cross_val_score(model_rf, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['HistGradient'] = mean(cross_val_score(model_hgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Voting'] = mean(cross_val_score(model_vot, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['GradBoost'] = mean(cross_val_score(model_gb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['NeuralN'] = mean(cross_val_score(model_nn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "\n",
    "  # XGBoost needs to LabelEncode the y before fitting the model\n",
    "  from sklearn.preprocessing import LabelEncoder\n",
    "  le = LabelEncoder().fit(y)\n",
    "  y_encoded = le.transform(y)\n",
    "  fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y_encoded, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "\n",
    "  # Add the model to another dict; make sure the keys have the same names as the list above\n",
    "  models['Logistic'] = model_log\n",
    "  models['Ridge'] = model_logcv\n",
    "  models['SGD'] = model_sgd\n",
    "  models['PassiveAggressive'] = model_pa\n",
    "  models['Perceptron'] = model_per\n",
    "  models['KNN'] = model_knn\n",
    "  models['SVM'] = model_svm\n",
    "  models['NaiveBayes'] = model_nb\n",
    "  models['Bagging'] = model_bag\n",
    "  models['AdaBoost'] = model_ada\n",
    "  models['ExtraTrees'] = model_ext\n",
    "  models['RandomForest'] = model_rf\n",
    "  models['HistGradient'] = model_hgb\n",
    "  models['Voting'] = model_vot\n",
    "  models['GradBoost'] = model_gb\n",
    "  models['XGBoost'] = model_xgb\n",
    "  models['NeuralN'] = model_nn\n",
    "\n",
    "  # Add the fit dictionary to a new DataFrame, sort, extract the top row, use it to retrieve the model object from the models dictionary\n",
    "  df_fit = pd.DataFrame({'Accuracy':fit})\n",
    "  df_fit.sort_values(by=['Accuracy'], ascending=False, inplace=True)\n",
    "  best_model = df_fit.index[0]\n",
    "  print(df_fit)\n",
    "\n",
    "  return models[best_model].fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "87c1496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cv_classification(df, k, label, repeat=True, algorithm='ensemble', random_state=1, messages=True):\n",
    "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "  import pandas as pd\n",
    "  from numpy import mean\n",
    "  X, y = Xandy(df, label)\n",
    "  X = dummy_code(X)\n",
    "  if repeat:  cv = RepeatedKFold(n_splits=k, n_repeats=5, random_state=12345)\n",
    "  else:       cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "  if algorithm == 'linear':\n",
    "    from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "    model1 = RidgeClassifier(random_state=random_state)\n",
    "    model2 = SGDClassifier(random_state=random_state)\n",
    "    score1 = mean(cross_val_score(model1, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    score2 = mean(cross_val_score(model2, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  elif algorithm == 'ensemble':\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "    model1 = RandomForestClassifier(random_state=random_state)\n",
    "    model2 = GradientBoostingClassifier(random_state=random_state)\n",
    "    score1 = mean(cross_val_score(model1, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    score2 = mean(cross_val_score(model2, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  else:\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    model1 = MLPClassifier(random_state=random_state, max_iter=10000)\n",
    "    model2 = KNeighborsClassifier()\n",
    "    score1 = mean(cross_val_score(model1, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    score2 = mean(cross_val_score(model2, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  if messages:\n",
    "    print('Accuracy', '{: <25}'.format(type(model1).__name__), round(score1, 4))\n",
    "    print('Accuracy', '{: <25}'.format(type(model2).__name__), round(score2, 4))\n",
    "  if score1 > score2: return model1.fit(X, y)\n",
    "  else:               return model2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "817dfb8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 50 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n50 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 665, in fit\n    y = self._encode_y(y=y, sample_weight=None)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 1520, in _encode_y\n    raise ValueError(\nValueError: y contains 1 class after sample_weight trimmed classes with zero weights, while a minimum of 2 classes are required.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m df \u001b[38;5;241m=\u001b[39m impute_KNN(df, label)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Select features and store a trained model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m fit_cv_classification(df, \u001b[38;5;241m10\u001b[39m, label, messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# We have to begin with a trained model\u001b[39;00m\n\u001b[0;32m     18\u001b[0m df_reduced \u001b[38;5;241m=\u001b[39m select_features(df\u001b[38;5;241m.\u001b[39mcopy(), label, model) \u001b[38;5;66;03m# Use that model to select features\u001b[39;00m\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m fit_cv_classification_expanded(df_reduced, label, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Retrain the model with the smaller feature set\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[80], line 20\u001b[0m, in \u001b[0;36mfit_cv_classification\u001b[1;34m(df, k, label, repeat, algorithm, random_state, messages)\u001b[0m\n\u001b[0;32m     18\u001b[0m   model2 \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier(random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m     19\u001b[0m   score1 \u001b[38;5;241m=\u001b[39m mean(cross_val_score(model1, X, y, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39mcv, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 20\u001b[0m   score2 \u001b[38;5;241m=\u001b[39m mean(cross_val_score(model2, X, y, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39mcv, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLPClassifier\n",
      "File \u001b[1;32mc:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    713\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    714\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    715\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    716\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    717\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    718\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    719\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    720\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    721\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m    722\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    723\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    724\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    725\u001b[0m )\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:443\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m    423\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    424\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    425\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    441\u001b[0m )\n\u001b[1;32m--> 443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "File \u001b[1;32mc:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 50 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n50 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 665, in fit\n    y = self._encode_y(y=y, sample_weight=None)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 1520, in _encode_y\n    raise ValueError(\nValueError: y contains 1 class after sample_weight trimmed classes with zero weights, while a minimum of 2 classes are required.\n"
     ]
    }
   ],
   "source": [
    "# Don't forget to mount Google Drive if you need it:\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Setting the label here since it is used in multiple function calls\n",
    "label = 'CONF'\n",
    "\n",
    "# Import the data\n",
    "df = import_data('cbb.csv', messages=False)\n",
    "\n",
    "# Clean/prepare the data\n",
    "df = bin_groups(df, messages=False)\n",
    "df = missing_drop(df, label)\n",
    "df = impute_KNN(df, label)\n",
    "\n",
    "# Select features and store a trained model\n",
    "model = fit_cv_classification(df, 10, label, messages=False) # We have to begin with a trained model\n",
    "df_reduced = select_features(df.copy(), label, model) # Use that model to select features\n",
    "model = fit_cv_classification_expanded(df_reduced, label, k=10, r=5)  # Retrain the model with the smaller feature set\n",
    "\n",
    "# Deployment pipeline\n",
    "dump_pickle(model, 'saved_model_clf.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dac1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later when a page loads that needs the predicted value(s):\n",
    "model = load_pickle('saved_model_clf.sav')\n",
    "\n",
    "pd.DataFrame({'Actual':df_reduced.SaleCondition, 'Predicted':model.predict(df_reduced.drop(columns=['SaleCondition']))}).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to mount Google Drive if you need it:\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Set constants\n",
    "label = '3P_O'\n",
    "random_state = 1\n",
    "\n",
    "# Import the data; BUT!!! Make sure to randomly select 100% of the data because it is\n",
    "# sorted by the label 3P_O and that will create a problem for hyperparameter tuning\n",
    "df = import_data('cbb.csv', messages=False)\n",
    "df = df.sample(frac=1, random_state=random_state)\n",
    "\n",
    "# Clean/prepare the data\n",
    "df = bin_groups(df, messages=False)\n",
    "df = missing_drop(df, label)\n",
    "df = impute_KNN(df, label)\n",
    "\n",
    "# Train an XGBRegressor model with no parameters set for comparison\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Train a model to use for selecting features\n",
    "X, y = Xandy(df, label)\n",
    "model = XGBRegressor(random_state=random_state).fit(X, y)\n",
    "\n",
    "# Use that model to select features\n",
    "df = select_features(df, label, model)\n",
    "\n",
    "# Retrain the model with the smaller feature set\n",
    "model = XGBRegressor(random_state=random_state).fit(X, y)\n",
    "\n",
    "# Create a cv object to calculate a cross-validated R2 score\n",
    "cv = KFold(n_splits=3, random_state=random_state, shuffle=True)\n",
    "X, y = Xandy(df, label)\n",
    "\n",
    "print(f'Baseline R2 for XGBRegressor model:\\t{mean(cross_val_score(model, X, y, scoring=\"r2\", cv=cv, n_jobs=-1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd477ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "# Create the parameter grid of all values you want to try\n",
    "params = {\n",
    "    \"booster\": ['gbtree', 'gblinear', 'dart'],\n",
    "    \"learning_rate\": [0.1, 0.3, 0.5],  # It accepts float [0,1] specifying learning rate for training process. Default = 0.3\n",
    "    \"objective\": ['reg:squarederror'], # List of possible values: https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
    "}\n",
    "\n",
    "# Create the hypertuning search object\n",
    "model_xgb = GridSearchCV(\n",
    "    XGBRegressor(random_state=random_state),\n",
    "    params,\n",
    "    n_jobs=-1, # Number of threads to use; -1 means use all available\n",
    "    scoring='r2', # Options: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    cv=KFold(n_splits=3), # Choose any type of cross_validation you want\n",
    "    verbose=2, # How much information to display in the results; options: 1, 2, or 3\n",
    "    refit=True # This saves the best-fitting model\n",
    "    )\n",
    "\n",
    "model_xgb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best parameters: {model_xgb.best_params_}')\n",
    "print(f'R-squared:\\t {model_xgb.best_score_}')\n",
    "\n",
    "print(f'All results:')\n",
    "pd.DataFrame(model_xgb.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best parameters: {model_xgb.best_params_}')\n",
    "print(f'R-squared:\\t {model_xgb.best_score_}')\n",
    "\n",
    "print(f'All results:')\n",
    "pd.DataFrame({\n",
    "    \"Parameters\":model_xgb.cv_results_['params'],\n",
    "    \"Mean Fit Score\":model_xgb.cv_results_['mean_test_score'],\n",
    "    \"Std Fit Score\":model_xgb.cv_results_['std_test_score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfec72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the best fitted model from the GridSearchCV object\n",
    "final_model = model_xgb.best_estimator_\n",
    "\n",
    "# Save and deploy it\n",
    "dump_pickle(final_model, \"best_model.sav\")\n",
    "\n",
    "# Predict against it\n",
    "final_model.predict(df.drop(columns=['3P_O']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb9287",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"booster\": ['gbtree'], # Default is gbtree\n",
    "    \"learning_rate\": [0.1],  # It accepts float [0,1] specifying learning rate for training process. Default = 0.3\n",
    "    \"objective\": ['reg:squarederror'], # List of possible values: https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
    "    \"max_depth\": [3, 4, 5, 6], # Must be between 3-10; default = 6\n",
    "    \"min_child_weight\": [1, 2, 3], # Default = 1\n",
    "}\n",
    "\n",
    "# Create the hypertuning object\n",
    "model_xgb = GridSearchCV(\n",
    "    XGBRegressor(random_state=random_state),\n",
    "    params,\n",
    "    n_jobs=-1, # Number of threads to use; -1 means use all available\n",
    "    scoring='r2', # Options: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    cv=KFold(n_splits=3), # Choose any type of cross_validation you want\n",
    "    verbose=2, # How much information to display in the results; options: 1, 2, or 3\n",
    "    refit=True # This saves the best-fitting model\n",
    "    )\n",
    "\n",
    "model_xgb.fit(df.drop(columns=['3P_O']), df.3P_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best parameters: {model_xgb.best_params_}')\n",
    "print(f'R-squared:\\t {model_xgb.best_score_}')\n",
    "\n",
    "print(f'All results:')\n",
    "pd.DataFrame({\n",
    "    \"Parameters\":model_xgb.cv_results_['params'],\n",
    "    \"Mean Fit Score\":model_xgb.cv_results_['mean_test_score'],\n",
    "    \"Std Fit Score\":model_xgb.cv_results_['std_test_score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"booster\": ['gbtree'], # Default is gbtree\n",
    "    \"learning_rate\": [0.1],  # It accepts float [0,1] specifying learning rate for training process. Default = 0.3\n",
    "    \"objective\": ['reg:squarederror'], # List of possible values: https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
    "    \"max_depth\": [3], # Must be between 3-10; default = 6\n",
    "    \"min_child_weight\": [1], # Default = 1\n",
    "    \"gamma\": [0, 0.1, 0.2], # Default = 0\n",
    "    \"subsample\": [0.8, 0.9, 1], # Default = 1\n",
    "    \"colsample_bytree\": [0.8, 1], # Default = 1\n",
    "    \"alpha\": [0, .001, 1, 100], # Default = 0\n",
    "}\n",
    "\n",
    "# Create the hypertuning object\n",
    "model_xgb = GridSearchCV(\n",
    "    XGBRegressor(),\n",
    "    params,\n",
    "    n_jobs=-1, # Number of threads to use; -1 means use all available\n",
    "    scoring='r2', # Options: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    cv=KFold(n_splits=3), # Choose any type of cross_validation you want\n",
    "    verbose=2, # How much information to display in the results; options: 1, 2, or 3\n",
    "    refit=True # This saves the best-fitting model\n",
    "    )\n",
    "\n",
    "model_xgb.fit(df.drop(columns=['3P_O']), df.3P_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best parameters: {model_xgb.best_params_}')\n",
    "print(f'R-squared:\\t {model_xgb.best_score_}')\n",
    "\n",
    "print(f'All results:')\n",
    "pd.DataFrame({\n",
    "    \"Parameters\":model_xgb.cv_results_['params'],\n",
    "    \"Mean Fit Score\":model_xgb.cv_results_['mean_test_score'],\n",
    "    \"Std Fit Score\":model_xgb.cv_results_['std_test_score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f62aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"booster\": ['gbtree', 'gblinear', 'dart'], # Default is gbtree\n",
    "    \"learning_rate\": [0.1, 0.3, 0.5],  # It accepts float [0,1] specifying learning rate for training process. Default = 0.3\n",
    "    \"objective\": ['reg:squarederror'], # List of possible values: https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
    "    \"max_depth\": [3, 6, 9], # Must be between 3-10; default = 6\n",
    "    \"min_child_weight\": [1, 2, 3], # Default = 1\n",
    "    \"gamma\": [0, 0.1, 0.2], # Default = 0\n",
    "    \"subsample\": [0.8, 0.9, 1], # Default = 1\n",
    "    \"colsample_bytree\": [0.8, 1], # Default = 1\n",
    "    \"alpha\": [0, .001, 1, 100], # Default = 0\n",
    "}\n",
    "\n",
    "# Create the hypertuning object\n",
    "model_xgb = GridSearchCV(\n",
    "    XGBRegressor(),\n",
    "    params,\n",
    "    n_jobs=-1, # Number of threads to use; -1 means use all available\n",
    "    scoring='r2', # Options: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    cv=KFold(n_splits=3), # Choose any type of cross_validation you want\n",
    "    verbose=2, # How much information to display in the results; options: 1, 2, or 3\n",
    "    refit=True # This saves the best-fitting model\n",
    "    )\n",
    "\n",
    "model_xgb.fit(df.drop(columns=['3P_O']), df.3P_O)\n",
    "\n",
    "# This will take too long; once you've started running the cell, just click the stop button and then proceed with the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f4aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you ever wait long enough for 17496 models to fit to see the R squared\n",
    "\n",
    "print(f'Best parameters: {model_xgb.best_params_}')\n",
    "print(f'R-squared:\\t {model_xgb.best_score_}')\n",
    "\n",
    "print(f'All results:')\n",
    "pd.DataFrame({\n",
    "    \"Parameters\":model_xgb.cv_results_['params'],\n",
    "    \"Mean Fit Score\":model_xgb.cv_results_['mean_test_score'],\n",
    "    \"Std Fit Score\":model_xgb.cv_results_['std_test_score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e384f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {\n",
    "    \"booster\": ['gbtree', 'gblinear', 'dart'], # Default is gbtree\n",
    "    \"learning_rate\": [0.1, 0.3, 0.5],  # It accepts float [0,1] specifying learning rate for training process. Default = 0.3\n",
    "    \"objective\": ['reg:squarederror'], # List of possible values: https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
    "    \"max_depth\": [3, 6, 9], # Must be between 3-10; default = 6\n",
    "    \"min_child_weight\": [1, 2, 3], # Default = 1\n",
    "    \"gamma\": [0, 0.1, 0.2], # Default = 0\n",
    "    \"subsample\": [0.8, 0.9, 1], # Default = 1\n",
    "    \"colsample_bytree\": [0.8, 1], # Default = 1\n",
    "    \"alpha\": [0, .001, 1, 100], # Default = 0\n",
    "}\n",
    "\n",
    "# Create the hypertuning object\n",
    "model_xgb = RandomizedSearchCV(\n",
    "    XGBRegressor(),\n",
    "    params,\n",
    "    n_iter=10, # Number of random samples to fit; default is 10\n",
    "    n_jobs=-1, # Number of threads to use; -1 means use all available\n",
    "    scoring='r2', # Options: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    cv=KFold(n_splits=3), # Choose any type of cross_validation you want\n",
    "    verbose=2, # How much information to display in the results; options: 1, 2, or 3\n",
    "    refit=True # This saves the best-fitting model\n",
    "    )\n",
    "\n",
    "model_xgb.fit(df.drop(columns=['3P_O']), df.3P_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c582f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best parameters: {model_xgb.best_params_}')\n",
    "print(f'R-squared:\\t {model_xgb.best_score_}')\n",
    "\n",
    "print(f'All results:')\n",
    "pd.DataFrame({\n",
    "    \"Parameters\":model_xgb.cv_results_['params'],\n",
    "    \"Mean Fit Score\":model_xgb.cv_results_['mean_test_score'],\n",
    "    \"Std Fit Score\":model_xgb.cv_results_['std_test_score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv # Must import this first\n",
    "from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "\n",
    "params = {\n",
    "    \"booster\": ['gbtree', 'gblinear', 'dart'], # Default is gbtree\n",
    "    \"learning_rate\": [0.1, 0.3, 0.5],  # It accepts float [0,1] specifying learning rate for training process. Default = 0.3\n",
    "    \"objective\": ['reg:squarederror'], # List of possible values: https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
    "    \"max_depth\": [3, 6, 9], # Must be between 3-10; default = 6\n",
    "    \"min_child_weight\": [1, 2, 3], # Default = 1\n",
    "    \"gamma\": [0, 0.1, 0.2], # Default = 0\n",
    "    \"subsample\": [0.8, 0.9, 1], # Default = 1\n",
    "    \"colsample_bytree\": [0.8, 1], # Default = 1\n",
    "    \"alpha\": [0, .001, 1, 100], # Default = 0\n",
    "}\n",
    "\n",
    "# Create the hypertuning object\n",
    "model_xgb = HalvingRandomSearchCV( # If this takes to long, change it to HalvingRandomSearchCV\n",
    "    XGBRegressor(),\n",
    "    params,\n",
    "    factor=2, # The 'halving' parameter; proportion of candidates selected for each iteration\n",
    "    n_candidates=32, # The number of hyperparameter value sets to randomly sample\n",
    "    resource='n_estimators', # Default = n_samples, but use n_estimators for boosting algorithms\n",
    "    n_jobs=-1, # Number of threads to use; -1 means use all available\n",
    "    scoring='r2', # Options: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    cv=KFold(n_splits=3), # Choose any type of cross_validation you want\n",
    "    verbose=2, # How much information to display in the results; options: 1, 2, or 3\n",
    "    max_resources=800, # The maximum number of resources (either n_samples or n_estimators) to use in each round\n",
    "    min_resources=50, # The maximum number of resources (either n_samples or n_estimators) to use in each round\n",
    "    refit=True # This saves the best-fitting model\n",
    "    )\n",
    "\n",
    "model_xgb.fit(df.drop(columns=['3P_O']), df.3P_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7942c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best parameters: {model_xgb.best_params_}')\n",
    "print(f'R-squared:\\t {model_xgb.best_score_}')\n",
    "\n",
    "print(f'All results:')\n",
    "pd.DataFrame({\n",
    "    \"Parameters\":model_xgb.cv_results_['params'],\n",
    "    \"Mean Fit Score\":model_xgb.cv_results_['mean_test_score'],\n",
    "    \"Std Fit Score\":model_xgb.cv_results_['std_test_score']\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
