{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ZyroY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.461, 'pos': 0.539, 'compound': 0.6893}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Word lists and lexicons in nltk: https://www.nltk.org/howto/corpus.html#word-lists-and-lexicons\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"This is a really great tweet!\")\n",
    "\n",
    "# Output:\n",
    "# [nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
    "# [nltk_data]   Package vader_lexicon is already up-to-date!\n",
    "# {'compound': 0.6893, 'neg': 0.0, 'neu': 0.461, 'pos': 0.539}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>context_annotations_count</th>\n",
       "      <th>count_annotations</th>\n",
       "      <th>count_cashtags</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>created_at_tweet</th>\n",
       "      <th>lang</th>\n",
       "      <th>...</th>\n",
       "      <th>location</th>\n",
       "      <th>protected</th>\n",
       "      <th>verified</th>\n",
       "      <th>media_type</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>preview_image_url</th>\n",
       "      <th>country</th>\n",
       "      <th>name_place</th>\n",
       "      <th>place_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1440484799970304000</td>\n",
       "      <td>This was my grandson this morning (w/autism)! ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-09-22T01:15:13.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>Victoria,  BC</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>photo</td>\n",
       "      <td>405</td>\n",
       "      <td>813</td>\n",
       "      <td>https://pbs.twimg.com/media/E_2hSs4UcAAIOK5.jpg</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Langford</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1439618825171963904</td>\n",
       "      <td>Wow!! Been into #York for the first time since...</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-09-19T15:54:09.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>Hessay, York</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>photo</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>https://pbs.twimg.com/media/E_qNsE1X0AQmoK_.jpg</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Hessay</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1248872872837332992</td>\n",
       "      <td>Sad number of ppl who lost life due to covid-1...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-04-11T07:17:50.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>Maidstone, South East</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>photo</td>\n",
       "      <td>288</td>\n",
       "      <td>278</td>\n",
       "      <td>https://pbs.twimg.com/media/EVTjQcoXsAAlrfq.jpg</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Maidstone</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1250729294051053568</td>\n",
       "      <td>Webinar now available‘Staying healthy at home ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-16T10:14:35.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>Maidstone, South East</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>photo</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>https://pbs.twimg.com/media/EVt7pYTXkAMGzxj.jpg</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Maidstone</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1249612131433095168</td>\n",
       "      <td>Webinar now available‘Staying healthy at home ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-13T08:15:23.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>Maidstone, South East</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>photo</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>https://pbs.twimg.com/media/EVeDlp7X0AMuN6X.jpg</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Maidstone</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1440484799970304000  This was my grandson this morning (w/autism)! ...   \n",
       "1  1439618825171963904  Wow!! Been into #York for the first time since...   \n",
       "2  1248872872837332992  Sad number of ppl who lost life due to covid-1...   \n",
       "3  1250729294051053568  Webinar now available‘Staying healthy at home ...   \n",
       "4  1249612131433095168  Webinar now available‘Staying healthy at home ...   \n",
       "\n",
       "   context_annotations_count  count_annotations  count_cashtags  \\\n",
       "0                          1                0.0             0.0   \n",
       "1                          2                2.0             0.0   \n",
       "2                          3                0.0             0.0   \n",
       "3                          1                2.0             0.0   \n",
       "4                          1                2.0             0.0   \n",
       "\n",
       "   count_hashtags  count_mentions  count_urls          created_at_tweet lang  \\\n",
       "0             0.0             0.0         1.0  2021-09-22T01:15:13.000Z   en   \n",
       "1             3.0             0.0         1.0  2021-09-19T15:54:09.000Z   en   \n",
       "2             0.0             0.0         1.0  2020-04-11T07:17:50.000Z   en   \n",
       "3             3.0             0.0         2.0  2020-04-16T10:14:35.000Z   en   \n",
       "4             3.0             0.0         2.0  2020-04-13T08:15:23.000Z   en   \n",
       "\n",
       "   ...               location  protected  verified  media_type height  width  \\\n",
       "0  ...         Victoria,  BC       False     False       photo    405    813   \n",
       "1  ...           Hessay, York      False     False       photo   2048   1536   \n",
       "2  ...  Maidstone, South East      False     False       photo    288    278   \n",
       "3  ...  Maidstone, South East      False     False       photo   2048   2048   \n",
       "4  ...  Maidstone, South East      False     False       photo   2048   2048   \n",
       "\n",
       "                                 preview_image_url         country name_place  \\\n",
       "0  https://pbs.twimg.com/media/E_2hSs4UcAAIOK5.jpg          Canada   Langford   \n",
       "1  https://pbs.twimg.com/media/E_qNsE1X0AQmoK_.jpg  United Kingdom     Hessay   \n",
       "2  https://pbs.twimg.com/media/EVTjQcoXsAAlrfq.jpg  United Kingdom  Maidstone   \n",
       "3  https://pbs.twimg.com/media/EVt7pYTXkAMGzxj.jpg  United Kingdom  Maidstone   \n",
       "4  https://pbs.twimg.com/media/EVeDlp7X0AMuN6X.jpg  United Kingdom  Maidstone   \n",
       "\n",
       "  place_type  \n",
       "0       city  \n",
       "1       city  \n",
       "2       city  \n",
       "3       city  \n",
       "4       city  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('tw_tweets_users_media_places.csv')\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types in the dataset:\n",
      "tweet_id                       int64\n",
      "text                          object\n",
      "context_annotations_count      int64\n",
      "count_annotations            float64\n",
      "count_cashtags               float64\n",
      "count_hashtags               float64\n",
      "count_mentions               float64\n",
      "count_urls                   float64\n",
      "created_at_tweet              object\n",
      "lang                          object\n",
      "likes                          int64\n",
      "quotes                         int64\n",
      "referenced_tweet_count         int64\n",
      "replies                        int64\n",
      "reply_settings                object\n",
      "retweets                       int64\n",
      "source                        object\n",
      "terms                         object\n",
      "username                      object\n",
      "created_at_author             object\n",
      "followers_count                int64\n",
      "following_count                int64\n",
      "tweet_count                    int64\n",
      "listed_count                   int64\n",
      "location                      object\n",
      "protected                       bool\n",
      "verified                        bool\n",
      "media_type                    object\n",
      "height                         int64\n",
      "width                          int64\n",
      "preview_image_url             object\n",
      "country                       object\n",
      "name_place                    object\n",
      "place_type                    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# First, check the data types\n",
    "print(\"\\nData types in the dataset:\")\n",
    "print(df.dtypes)\n",
    "# Select only numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Ensure 'text' column is string and handle NaNs\n",
    "df['text'] = df['text'].astype(str).fillna('')\n",
    "\n",
    "# Initialize sentiment columns\n",
    "df['sentiment_overall'] = 0.0\n",
    "df['sentiment_neg'] = 0.0\n",
    "df['sentiment_neu'] = 0.0\n",
    "df['sentiment_pos'] = 0.0\n",
    "\n",
    "# Perform sentiment analysis\n",
    "for row in df.itertuples():\n",
    "    sentiment = sia.polarity_scores(row.text)\n",
    "    df.loc[row.Index, 'sentiment_overall'] = sentiment['compound']\n",
    "    df.loc[row.Index, 'sentiment_neg'] = sentiment['neg']\n",
    "    df.loc[row.Index, 'sentiment_neu'] = sentiment['neu']\n",
    "    df.loc[row.Index, 'sentiment_pos'] = sentiment['pos']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select_dtypes(include=['number', 'float64', 'int64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               retweets   R-squared:                       0.058\n",
      "Model:                            OLS   Adj. R-squared:                  0.056\n",
      "Method:                 Least Squares   F-statistic:                     32.46\n",
      "Date:                Tue, 08 Apr 2025   Prob (F-statistic):           2.02e-08\n",
      "Time:                        20:47:50   Log-Likelihood:                -2333.5\n",
      "No. Observations:                 534   AIC:                             4671.\n",
      "Df Residuals:                     532   BIC:                             4680.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "tweet_id         3.061e-18   6.41e-19      4.775      0.000     1.8e-18    4.32e-18\n",
      "followers_count  8.242e-05   1.44e-05      5.742      0.000    5.42e-05       0.000\n",
      "const           -1.601e-12   2.79e-13     -5.742      0.000   -2.15e-12   -1.05e-12\n",
      "==============================================================================\n",
      "Omnibus:                     1037.167   Durbin-Watson:                   2.059\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           962925.245\n",
      "Skew:                          13.428   Prob(JB):                         0.00\n",
      "Kurtosis:                     209.292   Cond. No.                     2.97e+19\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.97e+19. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "y = df['retweets']\n",
    "X = df.drop(columns=['retweets', 'sentiment_overall', 'context_annotations_count', 'count_annotations', 'count_cashtags', 'count_hashtags', \n",
    "                     'count_mentions', 'count_urls', 'referenced_tweet_count', 'listed_count', 'width', 'height', 'tweet_count', \n",
    "                     'sentiment_neu', 'sentiment_neg', 'sentiment_pos', 'likes', 'quotes', 'replies', 'following_count']).assign(const=1)\n",
    "\n",
    "print(sm.OLS(y, X).fit().summary())\n",
    "\n",
    "# Output:\n",
    "# R-squared: 0.246"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### retweets=(−1.601 * 10^-12) + ((3.061 * 10^-18) * tweet_id) + ((8.242 * 10^-5) * followers_count)\n",
    "#### Our model goes as above\n",
    "\n",
    "#### The significant predictors of retweets are **followers_count** (p < 0.001) and **tweet_id** (p < 0.001). More followers correlate with more retweets, while tweet_id likely reflects time-based effects. Sentiment variables were not significant, as they were not included in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>context_annotations_count</th>\n",
       "      <th>count_annotations</th>\n",
       "      <th>count_cashtags</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>created_at_tweet</th>\n",
       "      <th>lang</th>\n",
       "      <th>...</th>\n",
       "      <th>location</th>\n",
       "      <th>protected</th>\n",
       "      <th>verified</th>\n",
       "      <th>media_type</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>preview_image_url</th>\n",
       "      <th>country</th>\n",
       "      <th>name_place</th>\n",
       "      <th>place_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1440484799970304000</td>\n",
       "      <td>This was my grandson this morning (w/autism)! ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-09-22T01:15:13.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>Victoria,  BC</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>photo</td>\n",
       "      <td>405</td>\n",
       "      <td>813</td>\n",
       "      <td>https://pbs.twimg.com/media/E_2hSs4UcAAIOK5.jpg</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Langford</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1439618825171963904</td>\n",
       "      <td>Wow!! Been into #York for the first time since...</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-09-19T15:54:09.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>Hessay, York</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>photo</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>https://pbs.twimg.com/media/E_qNsE1X0AQmoK_.jpg</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Hessay</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1248872872837332992</td>\n",
       "      <td>Sad number of ppl who lost life due to covid-1...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-04-11T07:17:50.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>Maidstone, South East</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>photo</td>\n",
       "      <td>288</td>\n",
       "      <td>278</td>\n",
       "      <td>https://pbs.twimg.com/media/EVTjQcoXsAAlrfq.jpg</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Maidstone</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1250729294051053568</td>\n",
       "      <td>Webinar now available‘Staying healthy at home ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-16T10:14:35.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>Maidstone, South East</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>photo</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>https://pbs.twimg.com/media/EVt7pYTXkAMGzxj.jpg</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Maidstone</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1249612131433095168</td>\n",
       "      <td>Webinar now available‘Staying healthy at home ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-13T08:15:23.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>Maidstone, South East</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>photo</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>https://pbs.twimg.com/media/EVeDlp7X0AMuN6X.jpg</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Maidstone</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1440484799970304000  This was my grandson this morning (w/autism)! ...   \n",
       "1  1439618825171963904  Wow!! Been into #York for the first time since...   \n",
       "2  1248872872837332992  Sad number of ppl who lost life due to covid-1...   \n",
       "3  1250729294051053568  Webinar now available‘Staying healthy at home ...   \n",
       "4  1249612131433095168  Webinar now available‘Staying healthy at home ...   \n",
       "\n",
       "   context_annotations_count  count_annotations  count_cashtags  \\\n",
       "0                          1                0.0             0.0   \n",
       "1                          2                2.0             0.0   \n",
       "2                          3                0.0             0.0   \n",
       "3                          1                2.0             0.0   \n",
       "4                          1                2.0             0.0   \n",
       "\n",
       "   count_hashtags  count_mentions  count_urls          created_at_tweet lang  \\\n",
       "0             0.0             0.0         1.0  2021-09-22T01:15:13.000Z   en   \n",
       "1             3.0             0.0         1.0  2021-09-19T15:54:09.000Z   en   \n",
       "2             0.0             0.0         1.0  2020-04-11T07:17:50.000Z   en   \n",
       "3             3.0             0.0         2.0  2020-04-16T10:14:35.000Z   en   \n",
       "4             3.0             0.0         2.0  2020-04-13T08:15:23.000Z   en   \n",
       "\n",
       "   ...               location  protected  verified  media_type height  width  \\\n",
       "0  ...         Victoria,  BC       False     False       photo    405    813   \n",
       "1  ...           Hessay, York      False     False       photo   2048   1536   \n",
       "2  ...  Maidstone, South East      False     False       photo    288    278   \n",
       "3  ...  Maidstone, South East      False     False       photo   2048   2048   \n",
       "4  ...  Maidstone, South East      False     False       photo   2048   2048   \n",
       "\n",
       "                                 preview_image_url         country name_place  \\\n",
       "0  https://pbs.twimg.com/media/E_2hSs4UcAAIOK5.jpg          Canada   Langford   \n",
       "1  https://pbs.twimg.com/media/E_qNsE1X0AQmoK_.jpg  United Kingdom     Hessay   \n",
       "2  https://pbs.twimg.com/media/EVTjQcoXsAAlrfq.jpg  United Kingdom  Maidstone   \n",
       "3  https://pbs.twimg.com/media/EVt7pYTXkAMGzxj.jpg  United Kingdom  Maidstone   \n",
       "4  https://pbs.twimg.com/media/EVeDlp7X0AMuN6X.jpg  United Kingdom  Maidstone   \n",
       "\n",
       "  place_type  \n",
       "0       city  \n",
       "1       city  \n",
       "2       city  \n",
       "3       city  \n",
       "4       city  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tw_tweets_users_media_places.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ZyroY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "    \n",
    "# Gensim\n",
    "import gensim, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 'co']) # After reviewing the LDA, return to add words that you want to eliminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets: 534\n",
      "Original tweets: 534\n"
     ]
    }
   ],
   "source": [
    "# Print total number of tweets\n",
    "print(f'Total tweets: {len(df)}')\n",
    "\n",
    "# Remove retweets by checking if 'RT @' appears in the 'text' column\n",
    "df_no_retweets = df[~df['text'].str.contains(\"RT @\", na=False)]\n",
    "\n",
    "# Print number of original tweets (after removing retweets)\n",
    "print(f'Original tweets: {len(df)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'was', 'my', 'grandson', 'this', 'morning', 'autism', 'apparently', 'he', 'is', 'going', 'through', 'bout', 'of', 'bronchitis', 'amp', 'pneumonia', 'he', 'always', 'gets', 'it', 'every', 'year', 'my', 'daughter', 'said', 'that', 'she', 'is', 'sure', 'she', 'will', 'have', 'to', 'take', 'him', 'to', 'the', 'hospital', 'tonight', 'but', 'if', 'they', 'try', 'to', 'test', 'him', 'for', 'covid', 'she', 'will', 'walk', 'out', 'https', 'co', 'krqjit']\n",
      "['wow', 'been', 'into', 'york', 'for', 'the', 'first', 'time', 'since', 'christmas', 'shopping', 'in', 'dec', 'took', 'our', 'daughter', 'cerys', 'to', 'the', 'catcafe', 'to', 'celebrate', 'her', 'th', 'birthday', 'perfectly', 'relaxed', 'quiet', 'covid', 'unfriendly', 'and', 'autism', 'friendly', 'she', 'was', 'in', 'her', 'happy', 'place', 'https', 'co', 'qzck', 'fze']\n",
      "['sad', 'number', 'of', 'ppl', 'who', 'lost', 'life', 'due', 'to', 'covid', 'is', 'far', 'more', 'than', 'the', 'number', 'that', 'it', 'says', 'on', 'the', 'news', 'those', 'who', 'pass', 'away', 'in', 'care', 'homes', 'supported', 'living', 'or', 'in', 'the', 'community', 'are', 'not', 'counted', 'give', 'thought', 'of', 'autistic', 'people', 'amp', 'with', 'learning', 'disabilities', 'every', 'life', 'matters', 'https', 'co', 'jj', 'bqeupxe']\n",
      "['webinar', 'now', 'available', 'staying', 'healthy', 'at', 'home', 'during', 'the', 'coronavirus', 'crisis', 'deliver', 'by', 'matthew', 'roberts', 'and', 'steve', 'hardy', 'autism', 'socialcare', 'https', 'co', 'vfthnrb', 'https', 'co', 'dkoxg', 'sjrm']\n",
      "['webinar', 'now', 'available', 'staying', 'healthy', 'at', 'home', 'during', 'the', 'coronavirus', 'crisis', 'deliver', 'by', 'matthew', 'roberts', 'and', 'steve', 'hardy', 'autism', 'socialcare', 'https', 'co', 'vfthnrb', 'https', 'co', 'auevngqt']\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub(r'\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub(r'\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(r\"'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
    "        yield sent\n",
    "\n",
    "# Convert each tweet to a list of cleaned words and add to a master list\n",
    "data = df_no_retweets['text'].values.tolist()  # Correct column access for df_no_retweets\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Print the first 5 cleaned tweet word lists\n",
    "for tweet in data_words[:5]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name util",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[163], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# !python -m spacy download en_core_web_sm\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m spacy\u001b[38;5;241m.\u001b[39mcli\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattributeruler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributeRuler\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdep_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DependencyParser\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medit_tree_lemmatizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditTreeLemmatizer\n",
      "File \u001b[1;32mc:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\attributeruler.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[1;32mc:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\spacy\\language.py:51\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipe_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze_pipes, print_pipe_analysis, validate_attrs\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     ConfigSchema,\n\u001b[0;32m     46\u001b[0m     ConfigSchemaInit,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     validate_init_settings,\n\u001b[0;32m     50\u001b[0m )\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n",
      "File \u001b[1;32mc:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\spacy\\scorer.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorphology\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Morphology\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc, Span, Token\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Example\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleFrozenList, get_lang_class\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# This lets us add type hints for mypy etc. without causing circular imports\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\spacy\\training\\__init__.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Corpus, JsonlCorpus, PlainTextCorpus  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Example, validate_examples, validate_get_examples  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgold_io\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m docs_to_json, read_json_file  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miob_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     biluo_tags_to_offsets,\n\u001b[0;32m     10\u001b[0m     biluo_tags_to_spans,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     tags_to_entities,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m console_logger\n",
      "File \u001b[1;32mc:\\Users\\ZyroY\\anaconda3\\Lib\\site-packages\\spacy\\training\\gold_io.pyx:6\u001b[0m, in \u001b[0;36minit spacy.training.gold_io\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name util"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    # remove stop words using list comprehension\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    # add bigrams and trigrams using list comprehension\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "            \n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])    # Load spacy, but we don't need the parser or NER (named entity extraction) modules\n",
    "        \n",
    "    # perform lemmatization and another round of stopword removal to catch any that were created by the lemmatization process\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "             \n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "      \n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'spacy_loggers' has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_ready \u001b[38;5;241m=\u001b[39m process_words(data_words)  \u001b[38;5;66;03m# processed Text Data!\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m data_ready[:\u001b[38;5;241m5\u001b[39m]:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tweet)\n",
      "Cell \u001b[1;32mIn[133], line 9\u001b[0m, in \u001b[0;36mprocess_words\u001b[1;34m(texts, stop_words, allowed_postags)\u001b[0m\n\u001b[0;32m      6\u001b[0m texts \u001b[38;5;241m=\u001b[39m [trigram_mod[bigram_mod[doc]] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m      8\u001b[0m texts_out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 9\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy_loggers\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m, disable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m'\u001b[39m])    \u001b[38;5;66;03m# Load spacy, but we don't need the parser or NER (named entity extraction) modules\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# perform lemmatization and another round of stopword removal to catch any that were created by the lemmatization process\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m texts:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'spacy_loggers' has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "data_ready = process_words(data_words)  # processed Text Data!\n",
    "for tweet in data_ready[:5]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_ready' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create Dictionary\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m id2word \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(data_ready)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m id2word\u001b[38;5;241m.\u001b[39miteritems():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(row)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_ready' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning  libmamba Added empty dependency for problem type SOLVER_RULE_UPDATE\n",
      "\n",
      "LibMambaUnsatisfiableError: Encountered problems while solving:\n",
      "  - package spacy-3.0.0-py36h9a8d428_0 requires python >=3.6,<3.7.0a0, but none of the providers can be installed\n",
      "\n",
      "Could not solve for environment specs\n",
      "The following packages are incompatible\n",
      "├─ pin-1 is installable and it requires\n",
      "│  └─ python 3.12.* , which can be installed;\n",
      "└─ spacy 3.0**  is not installable because there are no viable options\n",
      "   ├─ spacy [3.0.0|3.0.1|...|3.0.6] would require\n",
      "   │  └─ python >=3.6,<3.7.0a0 , which conflicts with any installable versions previously reported;\n",
      "   ├─ spacy [3.0.0|3.0.1|...|3.0.6] would require\n",
      "   │  └─ python >=3.7,<3.8.0a0 , which conflicts with any installable versions previously reported;\n",
      "   ├─ spacy [3.0.0|3.0.1|...|3.0.6] would require\n",
      "   │  └─ python >=3.8,<3.9.0a0 , which conflicts with any installable versions previously reported;\n",
      "   └─ spacy [3.0.0|3.0.1|...|3.0.6] would require\n",
      "      └─ python >=3.9,<3.10.0a0 , which conflicts with any installable versions previously reported.\n",
      "\n",
      "Pins seem to be involved in the conflict. Currently pinned specs:\n",
      " - python 3.12.* (labeled as 'pin-1')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "        \n",
    "for row in id2word.iteritems():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 4\n",
    "            \n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, # This is the corpus we created above\n",
    "                                                    id2word=id2word, # This is the dictionary we created above\n",
    "                                                    num_topics=topics,\n",
    "                                                    random_state=12345,\n",
    "                                                    chunksize=100,\n",
    "                                                    passes=5,\n",
    "                                                    per_word_topics=True)\n",
    "            \n",
    "ldatopics = lda_model.show_topics(formatted=False)\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit = pd.DataFrame(columns=['topics', 'perplexity', 'coherence'])\n",
    "\n",
    "for n in range(3,10):\n",
    "    # Fit LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                                      id2word=id2word, \n",
    "                                                      num_topics=n, \n",
    "                                                      random_state=12345,\n",
    "                                                      chunksize=100, \n",
    "                                                      passes=5,\n",
    "                                                      per_word_topics=True)\n",
    "        \n",
    "    # Generate fit metrics\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "    # Add metrics to df_fit\n",
    "    df_fit.loc[str(n - 3)] = [n, round(lda_model.log_perplexity(corpus), 3), round(coherence_model_lda.get_coherence(), 3)]\n",
    "        \n",
    "df_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "        \n",
    "df_fit['diff'] = abs(df_fit.coherence - df_fit.perplexity)\n",
    "sns.lineplot(x='topics', y='diff', data=df_fit, ci=None, marker='o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                              id2word=id2word, \n",
    "                                              num_topics=8, \n",
    "                                              random_state=1,\n",
    "                                              chunksize=100, \n",
    "                                              passes=5,\n",
    "                                              per_word_topics=True)\n",
    "\n",
    "#NOTICE THE ABOVE CODE IS SLIGHTLY DIFFERENT FROM YOUR BOOK\n",
    "\n",
    "df_topics = df_tweets.copy()\n",
    "        \n",
    "num_topics = len(lda_model.get_topics()) # store the number of topics from the last model\n",
    "for col in range(num_topics): # generate a new column for each topic\n",
    "    df_topics[f'topic_{col + 1}'] = 0.0\n",
    "          \n",
    "        # Store the topic score and dominant topic\n",
    "for i, words in enumerate(data_ready):\n",
    "    doc = lda_model[id2word.doc2bow(words)] # generate a corpus for this document set of words\n",
    "          \n",
    "    for j, score in enumerate(doc[0]): # for each document in the corpus\n",
    "            # Get the topic score and store it in the appropriate column\n",
    "        df_topics.iat[i, (len(df_topics.columns) - ((num_topics) - score[0]))] = score[1]\n",
    "        \n",
    "df_topics.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_topics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m pd\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mmax_colwidth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# Create the output DataFrame\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df_representative_tweets \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39mdf_topics\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# Iterate through each topic\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_topics \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      9\u001b[0m           \u001b[38;5;66;03m# Copy the row from the original df with the highest topic score into the new df\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_topics' is not defined"
     ]
    }
   ],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "        \n",
    "        # Create the output DataFrame\n",
    "df_representative_tweets = pd.DataFrame(columns=df_topics.columns)\n",
    "        \n",
    "        # Iterate through each topic\n",
    "for n in range(1, num_topics + 1):\n",
    "          # Copy the row from the original df with the highest topic score into the new df\n",
    "    df_representative_tweets.loc['topic_' + str(n)] = df_topics.loc[df_topics['topic_' + str(n)].idxmax()]\n",
    "        \n",
    "df_representative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of word counts for each topic\n",
    "from collections import Counter\n",
    "import matplotlib.colors as mcolors, math\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "  for word, weight in topic:\n",
    "    out.append([word, i + 1, weight, counter[word]])\n",
    "\n",
    "df_temp = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])\n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "matrix_size = math.ceil(num_topics**(1/2))  # Computes the n by n number of plots to generate\n",
    "fig, axes = plt.subplots(matrix_size, matrix_size, figsize=(20,20), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "  ax.bar(x='word', height=\"word_count\", data=df_temp.loc[df_temp.topic_id==i+1, :], color=cols[i+1], width=0.5, alpha=0.3, label='Word Count')\n",
    "  ax_twin = ax.twinx()\n",
    "  ax_twin.bar(x='word', height=\"importance\", data=df_temp.loc[df_temp.topic_id==i+1, :], color=cols[i+1], width=0.2, label='Weights')\n",
    "  ax.set_ylabel('Word Count', color=cols[i+1])\n",
    "  ax.set_title('Topic: ' + str(i + 1), color=cols[i+1], fontsize=16)\n",
    "  ax.tick_params(axis='y', left=False)\n",
    "  ax.set_xticks(ax.get_xticks())\n",
    "  ax.set_xticklabels(df_temp.loc[df_temp.topic_id==i+1, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "  ax.legend(loc='upper center'); ax_twin.legend(loc='upper right')\n",
    "  if i >= len(topics): # Turn off the unneeded subplots\n",
    "    ax.axis('off')\n",
    "    ax.title.set_visible(False)\n",
    "    ax_twin.axis('off')\n",
    "    ax.legend().set_visible(False)\n",
    "    ax_twin.legend().set_visible(False)\n",
    "\n",
    "fig.tight_layout(w_pad=2)\n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=20, y=1.03)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
